"""
Jupyter Notebook Linter Module
–ú–æ–¥—É–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–∏–Ω—Ç–µ—Ä–∞ –ø—Ä—è–º–æ –∏–∑ Jupyter Notebook
"""

import json
import os
from typing import List, Dict, Any, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from IPython.display import display, HTML, Markdown
import pandas as pd

class NotebookLinter:
    def __init__(self, model_name: str = "./–¢–µ–∫—Å—Ç–æ–≤—ã–µ/Qwen3-0.6B", max_tokens: int = 4000, 
                 max_new_tokens: int = 8192, temperature: float = 0.7):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Jupyter Notebook"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        self.max_tokens = max_tokens
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        
    def load_notebook(self, file_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ Jupyter Notebook –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                notebook = json.load(f)
            return notebook
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {str(e)}")
    
    def extract_cell_content(self, cell: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —è—á–µ–π–∫–∏"""
        cell_type = cell.get('cell_type', '')
        source = cell.get('source', [])
        
        if isinstance(source, list):
            content = ''.join(source)
        else:
            content = str(source)
            
        if cell_type == 'code':
            outputs = cell.get('outputs', [])
            output_text = self._extract_outputs(outputs)
            return f"```python\n{content}\n```\n\nOutputs:\n{output_text}"
        else:
            return content
    
    def _extract_outputs(self, outputs: List[Dict[str, Any]]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ outputs –∏–∑ —è—á–µ–π–∫–∏ –∫–æ–¥–∞"""
        output_texts = []
        for output in outputs:
            output_type = output.get('output_type', '')
            
            if output_type == 'stream':
                text = ''.join(output.get('text', []))
                output_texts.append(f"Stream: {text}")
            elif output_type == 'execute_result':
                data = output.get('data', {})
                if 'text/plain' in data:
                    text = ''.join(data['text/plain'])
                    output_texts.append(f"Result: {text}")
            elif output_type == 'error':
                error_name = output.get('ename', '')
                error_value = output.get('evalue', '')
                output_texts.append(f"Error: {error_name}: {error_value}")
        
        return '\n'.join(output_texts) if output_texts else "No outputs"
    
    def split_notebook_into_chunks(self, notebook: Dict[str, Any]) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏"""
        cells = notebook.get('cells', [])
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for i, cell in enumerate(cells):
            cell_content = self.extract_cell_content(cell)
            cell_tokens = len(self.tokenizer.encode(cell_content))
            
            if current_tokens + cell_tokens > self.max_tokens and current_chunk:
                chunk_text = self._format_chunk(current_chunk, i - len(current_chunk))
                chunks.append(chunk_text)
                
                current_chunk = [cell]
                current_tokens = cell_tokens
            else:
                current_chunk.append(cell)
                current_tokens += cell_tokens
        
        if current_chunk:
            chunk_text = self._format_chunk(current_chunk, len(cells) - len(current_chunk))
            chunks.append(chunk_text)
        
        return chunks
    
    def _format_chunk(self, cells: List[Dict[str, Any]], start_index: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–∏ —Ç–µ—Ç—Ä–∞–¥–∫–∏"""
        chunk_text = f"# –ß–∞—Å—Ç—å —Ç–µ—Ç—Ä–∞–¥–∫–∏ (—è—á–µ–π–∫–∏ {start_index + 1}-{start_index + len(cells)})\n\n"
        
        for i, cell in enumerate(cells, start_index + 1):
            cell_type = cell.get('cell_type', '')
            content = self.extract_cell_content(cell)
            
            if cell_type == 'markdown':
                chunk_text += f"## –Ø—á–µ–π–∫–∞ {i} (Markdown)\n{content}\n\n"
            else:
                chunk_text += f"## –Ø—á–µ–π–∫–∞ {i} (Code)\n{content}\n\n"
        
        return chunk_text
    
    def generate_improved_notebook(self, chunk: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ —á–∞—Å—Ç–∏ —Ç–µ—Ç—Ä–∞–¥–∫–∏"""
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—é Jupyter Notebook. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—É—é —á–∞—Å—Ç—å —Ç–µ—Ç—Ä–∞–¥–∫–∏ –∏ —É–ª—É—á—à–∏ –µ—ë:

1. –î–æ–±–∞–≤—å –ø–æ–Ω—è—Ç–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤
2. –û–±—Ä–∞–º–∏ –∫–æ–¥ –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
3. –ù–∞ –æ—Å–Ω–æ–≤–µ outputs –Ω–∞–ø–∏—à–∏ –≤—ã–≤–æ–¥—ã –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–∏
5. –°–¥–µ–ª–∞–π —Ç–µ—Ç—Ä–∞–¥–∫—É –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º–æ–π –∏ –ø–æ–Ω—è—Ç–Ω–æ–π

–ß–∞—Å—Ç—å —Ç–µ—Ç—Ä–∞–¥–∫–∏:
{chunk}

–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è:"""

        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=self.max_new_tokens,
            temperature=self.temperature,
            do_sample=True
        )
        
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        
        try:
            index = len(output_ids) - output_ids[::-1].index(151668)
        except ValueError:
            index = 0
        
        content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
        return content
    
    def process_notebook(self, file_path: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ—Ç—Ä–∞–¥–∫–∏"""
        print(f"–ó–∞–≥—Ä—É–∂–∞—é —Ç–µ—Ç—Ä–∞–¥–∫—É: {file_path}")
        notebook = self.load_notebook(file_path)
        
        print("–†–∞–∑–±–∏–≤–∞—é —Ç–µ—Ç—Ä–∞–¥–∫—É –Ω–∞ —á–∞—Å—Ç–∏...")
        chunks = self.split_notebook_into_chunks(notebook)
        
        print(f"–¢–µ—Ç—Ä–∞–¥–∫–∞ —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
        
        improved_parts = []
        for i, chunk in enumerate(chunks, 1):
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞—Å—Ç—å {i}/{len(chunks)}...")
            improved_chunk = self.generate_improved_notebook(chunk)
            improved_parts.append(improved_chunk)
        
        final_notebook = "\n\n" + "="*80 + "\n\n".join(improved_parts)
        return final_notebook
    
    def save_improved_notebook(self, improved_content: str, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–∏"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(improved_content)
        print(f"–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–µ—Ç—Ä–∞–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_path}")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ª–∏–Ω—Ç–µ—Ä–∞
_linter_instance = None

def init_linter(model_name: str = "./–¢–µ–∫—Å—Ç–æ–≤—ã–µ/Qwen3-0.6B", 
                max_tokens: int = 4000, 
                max_new_tokens: int = 8192, 
                temperature: float = 0.7) -> NotebookLinter:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–∏–Ω—Ç–µ—Ä–∞
    
    Args:
        model_name: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏
        max_new_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    
    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä NotebookLinter
    """
    global _linter_instance
    
    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Jupyter Notebook Linter...")
    print(f"üìÅ –ú–æ–¥–µ–ª—å: {model_name}")
    print(f"‚öôÔ∏è  –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —á–∞—Å—Ç—å: {max_tokens}")
    print(f"‚öôÔ∏è  –ú–∞–∫—Å–∏–º—É–º –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {max_new_tokens}")
    print(f"‚öôÔ∏è  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature}")
    
    _linter_instance = NotebookLinter(
        model_name=model_name,
        max_tokens=max_tokens,
        max_new_tokens=max_new_tokens,
        temperature=temperature
    )
    
    print("‚úÖ –õ–∏–Ω—Ç–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
    return _linter_instance

def get_linter() -> Optional[NotebookLinter]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ª–∏–Ω—Ç–µ—Ä–∞"""
    global _linter_instance
    if _linter_instance is None:
        print("‚ö†Ô∏è  –õ–∏–Ω—Ç–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ init_linter()")
        return None
    return _linter_instance

def lint_notebook(file_path: str, output_path: Optional[str] = None, 
                  display_result: bool = True) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ Jupyter Notebook —Ñ–∞–π–ª–∞
    
    Args:
        file_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É .ipynb
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        display_result: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —è—á–µ–π–∫–µ
    
    Returns:
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏
    """
    linter = get_linter()
    if linter is None:
        return ""
    
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ç—Ä–∞–¥–∫–∏
        improved_content = linter.process_notebook(file_path)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if output_path is None:
            base_name = os.path.splitext(file_path)[0]
            output_path = f"{base_name}_improved.md"
        
        linter.save_improved_notebook(improved_content, output_path)
        
        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if display_result:
            print("\n" + "="*60)
            print("üìÑ –†–ï–ó–£–õ–¨–¢–ê–¢ –û–ë–†–ê–ë–û–¢–ö–ò")
            print("="*60)
            display(Markdown(improved_content))
        
        return improved_content
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
        return ""

def analyze_notebook_structure(file_path: str) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Jupyter Notebook
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É .ipynb
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            notebook = json.load(f)
        
        cells = notebook.get('cells', [])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —è—á–µ–µ–∫
        cell_types = {}
        total_code_cells = 0
        total_markdown_cells = 0
        
        for cell in cells:
            cell_type = cell.get('cell_type', 'unknown')
            cell_types[cell_type] = cell_types.get(cell_type, 0) + 1
            
            if cell_type == 'code':
                total_code_cells += 1
                outputs = cell.get('outputs', [])
                if outputs:
                    total_markdown_cells += 1
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–æ–≤
        total_size = os.path.getsize(file_path)
        
        structure_info = {
            'total_cells': len(cells),
            'cell_types': cell_types,
            'code_cells': total_code_cells,
            'markdown_cells': total_markdown_cells,
            'cells_with_outputs': total_markdown_cells,
            'file_size_kb': total_size / 1024,
            'estimated_tokens': len(json.dumps(notebook)) // 4  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        }
        
        return structure_info
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {str(e)}")
        return {}

def display_notebook_info(file_path: str):
    """
    –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ Jupyter Notebook
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É .ipynb
    """
    info = analyze_notebook_structure(file_path)
    
    if not info:
        return
    
    print("üìä –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ NOTEBOOK")
    print("="*40)
    print(f"üìÅ –§–∞–π–ª: {file_path}")
    print(f"üìÑ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫: {info['total_cells']}")
    print(f"üíª –Ø—á–µ–π–∫–∏ –∫–æ–¥–∞: {info['code_cells']}")
    print(f"üìù Markdown —è—á–µ–π–∫–∏: {info['markdown_cells']}")
    print(f"üìä –Ø—á–µ–π–∫–∏ —Å –≤—ã–≤–æ–¥–∞–º–∏: {info['cells_with_outputs']}")
    print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {info['file_size_kb']:.1f} KB")
    print(f"üî§ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {info['estimated_tokens']}")
    
    # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    df_info = pd.DataFrame([
        ['–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫', info['total_cells']],
        ['–Ø—á–µ–π–∫–∏ –∫–æ–¥–∞', info['code_cells']],
        ['Markdown —è—á–µ–π–∫–∏', info['markdown_cells']],
        ['–Ø—á–µ–π–∫–∏ —Å –≤—ã–≤–æ–¥–∞–º–∏', info['cells_with_outputs']],
        ['–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (KB)', f"{info['file_size_kb']:.1f}"],
        ['–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤', info['estimated_tokens']]
    ], columns=['–ü–∞—Ä–∞–º–µ—Ç—Ä', '–ó–Ω–∞—á–µ–Ω–∏–µ'])
    
    display(df_info)

def quick_lint(file_path: str, model_name: str = "./–¢–µ–∫—Å—Ç–æ–≤—ã–µ/Qwen3-0.6B") -> str:
    """
    –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ç—Ä–∞–¥–∫–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É .ipynb
        model_name: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
    
    Returns:
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏
    """
    print("üöÄ –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ç—Ä–∞–¥–∫–∏...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–∏–Ω—Ç–µ—Ä–∞
    init_linter(model_name=model_name)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    return lint_notebook(file_path)

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—É—â–µ–π —Ç–µ—Ç—Ä–∞–¥–∫–æ–π
def get_current_notebook_path() -> Optional[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ —Ç–µ–∫—É—â–µ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)"""
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è Jupyter
        import ipykernel
        import requests
        from notebook.notebookapp import list_running_servers
        
        servers = list_running_servers()
        if servers:
            server = servers[0]
            url = f"{server['url']}api/sessions"
            response = requests.get(url, params={'token': server.get('token', '')})
            if response.status_code == 200:
                sessions = response.json()
                for session in sessions:
                    if session['kernel']['id'] == ipykernel.get_connection_file():
                        return session['notebook']['path']
    except:
        pass
    
    return None

def lint_current_notebook(output_path: Optional[str] = None) -> str:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—É—â–µ–π —Ç–µ—Ç—Ä–∞–¥–∫–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å)
    
    Args:
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    
    Returns:
        –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ—Ç—Ä–∞–¥–∫–∏
    """
    current_path = get_current_notebook_path()
    
    if current_path is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—É—Ç—å –∫ —Ç–µ–∫—É—â–µ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ lint_notebook() —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—É—Ç–∏")
        return ""
    
    print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–µ–∫—É—â—É—é —Ç–µ—Ç—Ä–∞–¥–∫—É: {current_path}")
    return lint_notebook(current_path, output_path)

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
def save_result_to_file(content: str, file_path: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ —Ñ–∞–π–ª"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {file_path}")

def display_result_as_markdown(content: str):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–∞–∫ Markdown"""
    display(Markdown(content))

def display_result_as_html(content: str):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–∞–∫ HTML"""
    html_content = f"""
    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 10px; border: 1px solid #dee2e6;">
        <h3 style="color: #495057; margin-top: 0;">–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏</h3>
        <div style="background-color: white; padding: 15px; border-radius: 5px; border: 1px solid #ced4da;">
            {content.replace(chr(10), '<br>')}
        </div>
    </div>
    """
    display(HTML(html_content)) 