#!/usr/bin/env python3
"""
Jupyter Notebook Linter - CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
–ü—Ä–æ—Å—Ç–æ–π –∫–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ª–∏–Ω—Ç–µ—Ä–∞
"""

import sys
import os
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from notebook_linter_module import process_notebook

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    print("Jupyter Notebook Linter - CLI")
    print("=" * 40)
    print("–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Jupyter Notebook –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ:")
    print("from notebook_linter_module import process_notebook")
    print("\n–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ—Ç CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
        
        if not os.path.exists(input_file):
            print(f"‚ùå –§–∞–π–ª '{input_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return
        
        if not input_file.endswith('.ipynb'):
            print("‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –§–∞–π–ª –Ω–µ –∏–º–µ–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è .ipynb")
        
        try:
            print(f"üìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {input_file}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
            model_name = "./–¢–µ–∫—Å—Ç–æ–≤—ã–µ/Qwen3-0.6B"
            
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto",
                device_map="auto"
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞
            print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ notebook...")
            improved_notebook = process_notebook(model, tokenizer, input_file)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_file = input_file.replace('.ipynb', '_improved.ipynb')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(improved_notebook, f, indent=2, ensure_ascii=False)
            
            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            input_size = os.path.getsize(input_file)
            output_size = os.path.getsize(output_file)
            print(f"üìä –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {input_size / 1024:.1f} KB")
            print(f"üìä –†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {output_size / 1024:.1f} KB")
            print(f"üìÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ: {len(improved_notebook['cells'])}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            return
    
    else:
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        print("\nüìù –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º:")
        
        # –ó–∞–ø—Ä–æ—Å —Ñ–∞–π–ª–∞
        input_file = input("–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ .ipynb —Ñ–∞–π–ª—É (–∏–ª–∏ Enter –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        
        if not input_file:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return
        
        if not os.path.exists(input_file):
            print(f"‚ùå –§–∞–π–ª '{input_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return
        
        try:
            print(f"\nüìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {input_file}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
            print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")
            model_name = "./–¢–µ–∫—Å—Ç–æ–≤—ã–µ/Qwen3-0.6B"
            
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto",
                device_map="auto"
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞
            print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ notebook...")
            improved_notebook = process_notebook(model, tokenizer, input_file)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            output_file = input_file.replace('.ipynb', '_improved.ipynb')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(improved_notebook, f, indent=2, ensure_ascii=False)
            
            print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
            print(f"üìÑ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è—á–µ–µ–∫ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–µ—Ç—Ä–∞–¥–∫–µ: {len(improved_notebook['cells'])}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")

if __name__ == "__main__":
    main()